{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question Answering BERT Testing Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "MODEL_NAME = \"dbmdz/bert-base-turkish-cased\"\n",
    "configuration = BertConfig()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    ## BERT encoder\n",
    "    encoder = TFBertModel.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # QA model\n",
    "    input_ids = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
    "    token_type_ids = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
    "    attention_mask = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
    "    embedding = encoder.bert(input_ids,\n",
    "                        token_type_ids=token_type_ids, \n",
    "                        attention_mask=attention_mask)[0]\n",
    "    \n",
    "    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n",
    "    start_logits = layers.Flatten()(start_logits)\n",
    "    \n",
    "    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n",
    "    end_logits = layers.Flatten()(end_logits)\n",
    "    \n",
    "    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
    "    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
    "    \n",
    "    model = keras.Model(\n",
    "        inputs=[input_ids, token_type_ids, attention_mask],\n",
    "        outputs=[start_probs, end_probs],\n",
    "    )\n",
    "    \n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optimizer = keras.optimizers.Adam(lr=5e-5)\n",
    "    model.compile(optimizer=optimizer, loss=[loss, loss])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "slow_tokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "splitted_model =  MODEL_NAME.split(\"/\")\n",
    "save_path = \"bert_base_turkish_cased/\"\n",
    "\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    \n",
    "slow_tokenizer.save_pretrained(save_path)\n",
    "tokenizer = BertWordPieceTokenizer(save_path + \"vocab.txt\", lowercase=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model and Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at dbmdz/bert-base-turkish-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at dbmdz/bert-base-turkish-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 512)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert (TFBertMainLayer)          TFBaseModelOutputWit 110617344   input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "start_logit (Dense)             (None, 512, 1)       768         bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "end_logit (Dense)               (None, 512, 1)       768         bert[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           start_logit[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 512)          0           end_logit[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 512)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 512)          0           flatten_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 110,618,880\n",
      "Trainable params: 110,618,880\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"bert_base_turkish_cased/weights/dbmdz-bert-base-turkish-cased_seqlen512_epochs10.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WikiElement:\n",
    "    def __init__(self, question, context):\n",
    "        self.question = question\n",
    "        self.context = context\n",
    "        \n",
    "    def preprocess(self):    \n",
    "        # tokenize context   \n",
    "        tokenized_context = tokenizer.encode(self.context)\n",
    "        \n",
    "        # tokenize question\n",
    "        tokenized_question = tokenizer.encode(self.question)\n",
    "\n",
    "        # create inputs       \n",
    "        input_ids = tokenized_context.ids + tokenized_question.ids[1:]\n",
    "        token_type_ids = [0] * len(tokenized_context.ids) + [1] * len(tokenized_question.ids[1:])\n",
    "            \n",
    "        attention_mask = [1] * len(input_ids)\n",
    "        \n",
    "        # padding for equal lenght sequence\n",
    "        padding_length = MAX_LEN - len(input_ids)\n",
    "        if padding_length > 0: # pad\n",
    "            input_ids = input_ids + ([0] * padding_length)\n",
    "            attention_mask = attention_mask + ([0] * padding_length) # len(input) [1] + padding [0]\n",
    "            token_type_ids = token_type_ids + ([0] * padding_length) # context [0] + question [1] + padding [0]\n",
    "        elif padding_length < 0:\n",
    "            return\n",
    "        \n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.context_token_to_char = tokenized_context.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_targets(element):\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "    }\n",
    "    \n",
    "    i = 0\n",
    "\n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key].append(getattr(element, key))\n",
    "            \n",
    "    for key in dataset_dict:\n",
    "        dataset_dict[key] = np.array(dataset_dict[key])\n",
    "        \n",
    "    x = [\n",
    "        dataset_dict[\"input_ids\"],\n",
    "        dataset_dict[\"token_type_ids\"],\n",
    "        dataset_dict[\"attention_mask\"],\n",
    "    ]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answer(question, context):\n",
    "    element = WikiElement(my_question, my_context)\n",
    "    element.preprocess()\n",
    "    \n",
    "    x = create_input_targets(element)\n",
    "    \n",
    "    pred_start, pred_end = model2.predict(x)\n",
    "    \n",
    "    start = np.argmax(pred_start)\n",
    "    end = np.argmax(pred_end)\n",
    "    offsets = element.context_token_to_char\n",
    "\n",
    "    pred_char_start = offsets[start][0]\n",
    "\n",
    "    if end < len(offsets):\n",
    "        pred_char_end = offsets[end][1]\n",
    "        pred_ans = element.context[pred_char_start:pred_char_end]\n",
    "    else:\n",
    "        pred_ans = element.context[pred_char_start:]\n",
    "\n",
    "    print(\"question: {}\\n\\npredicted_answer: {}\\n\\ncontext: {}\".format(element.question, pred_ans, element.context))\n",
    "    \n",
    "    result = {\"question\": element.question,\n",
    "             \"predicted_answer\": pred_ans,\n",
    "             \"context\": element.context}\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_context = \"Soy gaz veya asal gaz, standart şartlar altında her biri, diğer elementlere kıyasla daha düşük kimyasal reaktifliğe sahip, kokusuz, renksiz, tek atomlu gaz olan kimyasal element grubudur. Helyum (He), neon (Ne), argon (Ar), kripton (Kr), ksenon (Xe) ve radon (Rn) doğal olarak bulunan altı soy gazdır ve tamamı ametaldir. Her biri periyodik tablonun sırasıyla ilk altı periyodunda, 18. grubunda (8A) yer alır. Grupta yer alan oganesson (Og) için ise önceleri soy gaz olabileceği ihtimali üzerinde durulsa da günümüzde metalik görünümlü reaktif bir katı olduğu öngörülmektedir.\"\n",
    "my_question = \"Doğal olarak bulunan altı soy gaz nelerdir?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question: Doğal olarak bulunan altı soy gaz nelerdir?\n",
      "\n",
      "predicted_answer: Helyum (He), neon (Ne), argon (Ar), kripton (Kr), ksenon (Xe) ve radon (Rn)\n",
      "\n",
      "context: Soy gaz veya asal gaz, standart şartlar altında her biri, diğer elementlere kıyasla daha düşük kimyasal reaktifliğe sahip, kokusuz, renksiz, tek atomlu gaz olan kimyasal element grubudur. Helyum (He), neon (Ne), argon (Ar), kripton (Kr), ksenon (Xe) ve radon (Rn) doğal olarak bulunan altı soy gazdır ve tamamı ametaldir. Her biri periyodik tablonun sırasıyla ilk altı periyodunda, 18. grubunda (8A) yer alır. Grupta yer alan oganesson (Og) için ise önceleri soy gaz olabileceği ihtimali üzerinde durulsa da günümüzde metalik görünümlü reaktif bir katı olduğu öngörülmektedir.\n"
     ]
    }
   ],
   "source": [
    "result = predict_answer(my_question, my_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\ProgramFiles\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: bert_base_turkish_cased/dbmdz-bert-base-turkish-cased_seqlen512_epochs10\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"bert_base_turkish_cased/dbmdz-bert-base-turkish-cased_seqlen512_epochs10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hata için buna bak [github](https://github.com/huggingface/transformers/issues/3627)\n",
    "\n",
    "Modeli model dosyası olarak kaydet, yoksa yüklemesi baya uzun sürüyor.\n",
    "Arkaplanda çalışan bir servis olsun. Bert, Electra, Albert.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.load_model('bert_base_turkish_cased/dbmdz-bert-base-turkish-cased_seqlen512_epochs10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
